{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# #GAN Model:\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
    "# from tensorflow.keras.layers import BatchNormalization, LeakyReLU\n",
    "# from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "\n",
    "#1. Resample Data before adding to discriminator\n",
    "#2. Add in Generator\n",
    "#3. Increase Number of Epochs\n",
    "#4. Analyze ROC and recall-precision curves\n",
    "#5. Hyperparametrization (if it's even possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    21204\n",
       "1      346\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data = pd.read_csv(\"fraud_data.csv\")\n",
    "fraud_data.drop_duplicates(inplace=True)\n",
    "fraud_data.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.from_numpy(np.array(fraud_data.iloc[0,1]).astype('float32'))#.unsqueeze(dim = 0).size()\n",
    "lambda x: torch.from_numpy(np.array(x).astype('float32')).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.983975\n",
       "1    0.016025\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.983853\n",
       "1    0.016147\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_strat = np.hstack([np.ones(shape = 5000), np.zeros(shape = fraud_data.shape[0]-5000)])\n",
    "df_train, df_test = train_test_split(fraud_data, test_size=0.25, stratify = fraud_data.Class)\n",
    "display(df_train.Class.value_counts(normalize = True), df_test.Class.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7702301410541945"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train.query(\"index <= 5000\").shape[0]/df_train.query(\"index > 5000\").shape[0]\n",
    "df_test.query(\"index > 5000\").shape[0]/df_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PdToTensor(df=[4, 5, 6], data=[1, 2, 3], labels=[1, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PdToTensor = namedtuple(\"PdToTensor\",[\"df\",\"data\",\"labels\"])\n",
    "m = PdToTensor([4,5,6],[1,2,3],[1,0,0])\n",
    "m\n",
    "\n",
    "# transform = torch.from_numpy,\n",
    "# target_transform = lambda x: torch.from_numpy(np.array(x).astype('float32')).unsqueeze(dim=0),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PdToTensor = namedtuple(\"PdToTensor\",[\"df\",\"sample\",\"data\",\"labels\"])\n",
    "\n",
    "class CustomPandasTorch(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # self.df = df.astype('float32') \n",
    "        self.raw = CustomPandasTorch.transform_data(df)\n",
    "        self.active = self.raw\n",
    "        # df_sample = (df\n",
    "        #     .groupby('Class')\n",
    "        #     [df.columns]\n",
    "        #     .apply(lambda x: x.sample(sample_size, random_state = 6, replace = True))\n",
    "        #     .sample(frac=1, random_state = 6)\n",
    "        #     .reset_index(drop = True)\n",
    "        # )\n",
    "        # self.sample = CustomPandasTorch.transform_data(df_sample)\n",
    "\n",
    "        df_train, df_test = train_test_split(df, test_size=0.25, stratify = df.Class)\n",
    "        self.train, self.test = CustomPandasTorch.transform_data(df_train), CustomPandasTorch.transform_data(df_test)\n",
    "\n",
    "        # self.sample_size = sample_size\n",
    "\n",
    "        # if transform_type == \"rs\":\n",
    "            #self.df = self.pd_sample(self.df)\n",
    "        #elif transform_type == \"split\":        \n",
    "            # self.df, self.df_test = train_test_split(self.df, test_size=0.25, stratify = fraud_data.Class)\n",
    "            #self.df = self.pd_sample(self.df)\n",
    "            # self.data_test = self.df_test.iloc[:,:-1]\n",
    "            # self.label_test = self.df_test.iloc[:,-1]\n",
    "            \n",
    "        # self.data, self.label = CustomPandasTorch.extractXy(self.df)\n",
    "\n",
    "    def transform_data(df_input):\n",
    "        sample_size = max((df_input.shape[0]//BATCH_SIZE+1)*BATCH_SIZE, 16000)\n",
    "        df = (df_input\n",
    "                .groupby('Class')\n",
    "                [df_input.columns]\n",
    "                .apply(lambda x: x.sample(sample_size, random_state = 6, replace = True))\n",
    "                .sample(frac=1, random_state = 6)\n",
    "                .reset_index(drop = True)        \n",
    "        )\n",
    "\n",
    "        t = torch.from_numpy(df.astype('float32').values)\n",
    "        return PdToTensor(df_input, df, t[:,:-1], t[:,-1:])\n",
    "    \n",
    "    def set_raw(self):\n",
    "        self.active = self.raw\n",
    "    \n",
    "    def set_train(self):\n",
    "        self.active = self.train\n",
    "\n",
    "    def set_test(self):\n",
    "        self.active = self.test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.active.sample.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # data = self.data.iloc[idx,:]\n",
    "        # label = self.label.iloc[idx]\n",
    "\n",
    "        # if self.transform:\n",
    "        #     data = self.transform(data.values)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return self.active.data[idx], self.active.labels[idx]\n",
    "\n",
    "    # def get_sample(self, size = 1000):\n",
    "    #     df_resampled = (self.data\n",
    "    #         .groupby(self.label)\n",
    "    #         .apply(lambda x: x.sample(size, random_state = 6, replace = True))\n",
    "    #         .reset_index(level = 0)\n",
    "    #         .sample(frac=1, random_state = 6)\n",
    "    #     )\n",
    "\n",
    "    #     data_rs = self.data.reindex(df_resampled.index)\n",
    "    #     label_rs = self.label.reindex(df_resampled.index)\n",
    "\n",
    "    #     return self.transform_data(data_rs, label_rs)\n",
    "    \n",
    "    # def pd_sample(self, df):\n",
    "    #     data = df.iloc[:,:-1]\n",
    "    #     label = df.iloc[:,-1]\n",
    "\n",
    "    #     return (data\n",
    "    #         .groupby(label)\n",
    "    #         .apply(lambda x: x.sample(self.sample_size, random_state = 6, replace = True))\n",
    "    #         .reset_index(level = 0)\n",
    "    #         .sample(frac=1, random_state = 6)\n",
    "    #         .reindex(columns = df.columns)\n",
    "    #         # .astype('float32')\n",
    "    #         .reset_index(drop = True)\n",
    "    #     )\n",
    "    \n",
    "    # def extractXy(df):\n",
    "    #     return df.iloc[:,:-1], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13f186730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.927453</td>\n",
       "      <td>1.827621</td>\n",
       "      <td>-7.019495</td>\n",
       "      <td>5.348303</td>\n",
       "      <td>-2.739188</td>\n",
       "      <td>-2.107219</td>\n",
       "      <td>-5.015848</td>\n",
       "      <td>1.205868</td>\n",
       "      <td>-4.382713</td>\n",
       "      <td>-8.337707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376938</td>\n",
       "      <td>-0.792017</td>\n",
       "      <td>-0.771414</td>\n",
       "      <td>-0.379574</td>\n",
       "      <td>0.718717</td>\n",
       "      <td>1.111151</td>\n",
       "      <td>1.277707</td>\n",
       "      <td>0.819081</td>\n",
       "      <td>512.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.432554</td>\n",
       "      <td>1.861373</td>\n",
       "      <td>-4.310353</td>\n",
       "      <td>2.448080</td>\n",
       "      <td>4.574094</td>\n",
       "      <td>-2.979912</td>\n",
       "      <td>-2.792379</td>\n",
       "      <td>-2.719867</td>\n",
       "      <td>-0.276704</td>\n",
       "      <td>-2.314747</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.384477</td>\n",
       "      <td>-0.348904</td>\n",
       "      <td>-3.979948</td>\n",
       "      <td>-0.828156</td>\n",
       "      <td>-2.419446</td>\n",
       "      <td>-0.767070</td>\n",
       "      <td>0.387039</td>\n",
       "      <td>0.319402</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.908637</td>\n",
       "      <td>2.849024</td>\n",
       "      <td>-5.647343</td>\n",
       "      <td>6.009415</td>\n",
       "      <td>0.216656</td>\n",
       "      <td>-2.397014</td>\n",
       "      <td>-1.819308</td>\n",
       "      <td>0.338527</td>\n",
       "      <td>-2.819883</td>\n",
       "      <td>-4.063098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407260</td>\n",
       "      <td>-0.397435</td>\n",
       "      <td>-0.080006</td>\n",
       "      <td>-0.168597</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.210510</td>\n",
       "      <td>0.648705</td>\n",
       "      <td>0.360224</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-22.341889</td>\n",
       "      <td>15.536133</td>\n",
       "      <td>-22.865228</td>\n",
       "      <td>7.043374</td>\n",
       "      <td>-14.183129</td>\n",
       "      <td>-0.463145</td>\n",
       "      <td>-28.215112</td>\n",
       "      <td>-14.607791</td>\n",
       "      <td>-9.481456</td>\n",
       "      <td>-20.949192</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.110423</td>\n",
       "      <td>4.158895</td>\n",
       "      <td>1.412928</td>\n",
       "      <td>0.382801</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>-0.632816</td>\n",
       "      <td>-4.380154</td>\n",
       "      <td>-0.467863</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-2.880042</td>\n",
       "      <td>5.225442</td>\n",
       "      <td>-11.063330</td>\n",
       "      <td>6.689951</td>\n",
       "      <td>-5.759924</td>\n",
       "      <td>-2.244031</td>\n",
       "      <td>-11.199975</td>\n",
       "      <td>4.014722</td>\n",
       "      <td>-3.429304</td>\n",
       "      <td>-11.561950</td>\n",
       "      <td>...</td>\n",
       "      <td>2.002883</td>\n",
       "      <td>0.351102</td>\n",
       "      <td>0.795255</td>\n",
       "      <td>-0.778379</td>\n",
       "      <td>-1.646815</td>\n",
       "      <td>0.487539</td>\n",
       "      <td>1.427713</td>\n",
       "      <td>0.583172</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>-22.561699</td>\n",
       "      <td>13.208904</td>\n",
       "      <td>-24.643819</td>\n",
       "      <td>6.232532</td>\n",
       "      <td>-16.905611</td>\n",
       "      <td>-4.497439</td>\n",
       "      <td>-16.810184</td>\n",
       "      <td>14.955107</td>\n",
       "      <td>-3.871297</td>\n",
       "      <td>-8.581266</td>\n",
       "      <td>...</td>\n",
       "      <td>1.765987</td>\n",
       "      <td>-1.635517</td>\n",
       "      <td>-0.998317</td>\n",
       "      <td>0.138972</td>\n",
       "      <td>1.559350</td>\n",
       "      <td>-0.222125</td>\n",
       "      <td>1.504425</td>\n",
       "      <td>0.445920</td>\n",
       "      <td>99.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21292</th>\n",
       "      <td>-7.381547</td>\n",
       "      <td>-7.449015</td>\n",
       "      <td>-4.696287</td>\n",
       "      <td>3.728439</td>\n",
       "      <td>6.198304</td>\n",
       "      <td>-6.406267</td>\n",
       "      <td>-5.831452</td>\n",
       "      <td>1.457175</td>\n",
       "      <td>-0.646203</td>\n",
       "      <td>-4.029129</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176575</td>\n",
       "      <td>-0.978692</td>\n",
       "      <td>-0.278330</td>\n",
       "      <td>-0.635874</td>\n",
       "      <td>0.123539</td>\n",
       "      <td>0.404729</td>\n",
       "      <td>0.704915</td>\n",
       "      <td>-1.229992</td>\n",
       "      <td>35.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21325</th>\n",
       "      <td>-3.632809</td>\n",
       "      <td>5.437263</td>\n",
       "      <td>-9.136521</td>\n",
       "      <td>10.307226</td>\n",
       "      <td>-5.421830</td>\n",
       "      <td>-2.864815</td>\n",
       "      <td>-10.634088</td>\n",
       "      <td>3.018127</td>\n",
       "      <td>-4.891640</td>\n",
       "      <td>-11.235048</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309880</td>\n",
       "      <td>0.978660</td>\n",
       "      <td>-0.096130</td>\n",
       "      <td>0.432377</td>\n",
       "      <td>-0.435628</td>\n",
       "      <td>0.650893</td>\n",
       "      <td>1.693608</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21406</th>\n",
       "      <td>-1.410852</td>\n",
       "      <td>2.268271</td>\n",
       "      <td>-2.297554</td>\n",
       "      <td>1.871331</td>\n",
       "      <td>0.248957</td>\n",
       "      <td>-1.208799</td>\n",
       "      <td>-1.358648</td>\n",
       "      <td>1.102916</td>\n",
       "      <td>-1.317364</td>\n",
       "      <td>-4.626919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155381</td>\n",
       "      <td>-0.614880</td>\n",
       "      <td>-0.196126</td>\n",
       "      <td>-0.464376</td>\n",
       "      <td>0.118473</td>\n",
       "      <td>-0.484537</td>\n",
       "      <td>0.373596</td>\n",
       "      <td>0.187657</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21656</th>\n",
       "      <td>0.467992</td>\n",
       "      <td>1.100118</td>\n",
       "      <td>-5.607145</td>\n",
       "      <td>2.204714</td>\n",
       "      <td>-0.578539</td>\n",
       "      <td>-0.174200</td>\n",
       "      <td>-3.454201</td>\n",
       "      <td>1.102823</td>\n",
       "      <td>-1.065016</td>\n",
       "      <td>-5.416037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983481</td>\n",
       "      <td>0.899876</td>\n",
       "      <td>-0.285103</td>\n",
       "      <td>-1.929717</td>\n",
       "      <td>0.319869</td>\n",
       "      <td>0.170636</td>\n",
       "      <td>0.851798</td>\n",
       "      <td>0.372098</td>\n",
       "      <td>120.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1         V2         V3         V4         V5        V6  \\\n",
       "15     -1.927453   1.827621  -7.019495   5.348303  -2.739188 -2.107219   \n",
       "149     0.432554   1.861373  -4.310353   2.448080   4.574094 -2.979912   \n",
       "182     0.908637   2.849024  -5.647343   6.009415   0.216656 -2.397014   \n",
       "255   -22.341889  15.536133 -22.865228   7.043374 -14.183129 -0.463145   \n",
       "296    -2.880042   5.225442 -11.063330   6.689951  -5.759924 -2.244031   \n",
       "...          ...        ...        ...        ...        ...       ...   \n",
       "21258 -22.561699  13.208904 -24.643819   6.232532 -16.905611 -4.497439   \n",
       "21292  -7.381547  -7.449015  -4.696287   3.728439   6.198304 -6.406267   \n",
       "21325  -3.632809   5.437263  -9.136521  10.307226  -5.421830 -2.864815   \n",
       "21406  -1.410852   2.268271  -2.297554   1.871331   0.248957 -1.208799   \n",
       "21656   0.467992   1.100118  -5.607145   2.204714  -0.578539 -0.174200   \n",
       "\n",
       "              V7         V8        V9        V10  ...       V21       V22  \\\n",
       "15     -5.015848   1.205868 -4.382713  -8.337707  ...  1.376938 -0.792017   \n",
       "149    -2.792379  -2.719867 -0.276704  -2.314747  ... -1.384477 -0.348904   \n",
       "182    -1.819308   0.338527 -2.819883  -4.063098  ...  0.407260 -0.397435   \n",
       "255   -28.215112 -14.607791 -9.481456 -20.949192  ... -9.110423  4.158895   \n",
       "296   -11.199975   4.014722 -3.429304 -11.561950  ...  2.002883  0.351102   \n",
       "...          ...        ...       ...        ...  ...       ...       ...   \n",
       "21258 -16.810184  14.955107 -3.871297  -8.581266  ...  1.765987 -1.635517   \n",
       "21292  -5.831452   1.457175 -0.646203  -4.029129  ...  1.176575 -0.978692   \n",
       "21325 -10.634088   3.018127 -4.891640 -11.235048  ...  2.309880  0.978660   \n",
       "21406  -1.358648   1.102916 -1.317364  -4.626919  ...  0.155381 -0.614880   \n",
       "21656  -3.454201   1.102823 -1.065016  -5.416037  ...  0.983481  0.899876   \n",
       "\n",
       "            V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "15    -0.771414 -0.379574  0.718717  1.111151  1.277707  0.819081  512.25   \n",
       "149   -3.979948 -0.828156 -2.419446 -0.767070  0.387039  0.319402    1.00   \n",
       "182   -0.080006 -0.168597  0.465058  0.210510  0.648705  0.360224    1.18   \n",
       "255    1.412928  0.382801  0.447154 -0.632816 -4.380154 -0.467863    1.00   \n",
       "296    0.795255 -0.778379 -1.646815  0.487539  1.427713  0.583172    1.00   \n",
       "...         ...       ...       ...       ...       ...       ...     ...   \n",
       "21258 -0.998317  0.138972  1.559350 -0.222125  1.504425  0.445920   99.99   \n",
       "21292 -0.278330 -0.635874  0.123539  0.404729  0.704915 -1.229992   35.00   \n",
       "21325 -0.096130  0.432377 -0.435628  0.650893  1.693608  0.857685    8.54   \n",
       "21406 -0.196126 -0.464376  0.118473 -0.484537  0.373596  0.187657    1.00   \n",
       "21656 -0.285103 -1.929717  0.319869  0.170636  0.851798  0.372098  120.54   \n",
       "\n",
       "       Class  \n",
       "15         1  \n",
       "149        1  \n",
       "182        1  \n",
       "255        1  \n",
       "296        1  \n",
       "...      ...  \n",
       "21258      1  \n",
       "21292      1  \n",
       "21325      1  \n",
       "21406      1  \n",
       "21656      1  \n",
       "\n",
       "[346 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.query(\"Class == 1\")#.sample(512, replace = True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGroups = namedtuple(\"DataGroups\", [\"all\", \"fraud\", \"valid\"])\n",
    "# train_fraud = fraud_data.sample(n = 8000).astype('float32')\n",
    "# train_fraud = CustomPandasTorch.pd_sample(fraud_data, size = 4000)\n",
    "# train_fraud_torch = CustomPandasTorch(\n",
    "#                                         fraud_data.query(\"Class == 1\")#.sample(512, replace = True).reset_index(drop=True)\n",
    "#                                         # transform = torch.from_numpy,\n",
    "#                                         # target_transform = lambda x: torch.from_numpy(np.array(x).astype('float32')).unsqueeze(dim=0),\n",
    "#                                         # transform_type = None\n",
    "#                                     )\n",
    "\n",
    "dg = DataGroups(CustomPandasTorch(fraud_data), CustomPandasTorch(fraud_data.query(\"Class == 1\")), CustomPandasTorch(fraud_data.query(\"Class == 0\")))\n",
    "#TODO: April 2 create 3 train_fraud_torch dataset (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_fraud_torch, batch_size=BATCH_SIZE, shuffle=True\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg.fraud, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 29])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader.dataset.set_raw()\n",
    "data, label = next(iter(train_loader))\n",
    "display(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(29, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(x.size(0), 784)\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 29),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        # output = output.view(x.size(0), 1, 28, 28)\n",
    "        return output\n",
    "\n",
    "# discriminator = Discriminator().to(device=device)\n",
    "# generator = Generator().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "num_epochs = 50\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "# optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(train_loader):\n",
    "    lr = 0.0001\n",
    "    num_epochs = 50\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    \n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # Training the discriminator\n",
    "            discriminator.zero_grad()\n",
    "            loss_discriminator = loss_function(discriminator(real_samples), real_labels)\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # Show loss\n",
    "            if n == BATCH_SIZE - 1:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "\n",
    "    return discriminator, None\n",
    "\n",
    "def train_gan(train_loader):\n",
    "    lr = 0.0001\n",
    "    num_epochs = 50\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    \n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # Data for training discriminator\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "            gen_labels = torch.zeros((BATCH_SIZE, 1))\n",
    "            all_samples = torch.cat((real_samples, gen_samples))\n",
    "            all_labels = torch.cat((real_labels, gen_labels))\n",
    "\n",
    "            # Training the discriminator\n",
    "            discriminator.zero_grad()\n",
    "            loss_discriminator = loss_function(discriminator(all_samples), all_labels)\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # Data and training the generator\n",
    "            generator.zero_grad()\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "            loss_generator = loss_function(discriminator(gen_samples), real_labels) \n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # Show loss\n",
    "            if n == BATCH_SIZE - 1:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "                print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
    "\n",
    "    return discriminator, generator\n",
    "\n",
    "\n",
    "#Recall Calc:\n",
    "def recall_calc(y_pred, y_test):\n",
    "    return (((y_pred==1)*(y_test==1)).to(torch.float32).sum())/((y_test==1).to(torch.float32).sum())\n",
    "\n",
    "# discriminator, generator = train_gan(train_loader)\n",
    "# recall_calc(discriminator(train_loader.dataset.active.data), train_loader.dataset.active.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell takes ~4mins to run\n",
    "result_lst = []\n",
    "\n",
    "for i, (t, func) in enumerate(zip([dg.all, dg.fraud], [train_discriminator, train_gan])):\n",
    "    print(i)\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.active.data), t.active.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"all\"])\n",
    "    t.set_train()\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.test.data), t.test.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"train\"])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>df_size</th>\n",
       "      <th>disc</th>\n",
       "      <th>gen</th>\n",
       "      <th>df_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.8035)</td>\n",
       "      <td>43136</td>\n",
       "      <td>21550</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.7696)</td>\n",
       "      <td>32384</td>\n",
       "      <td>16162</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.9937)</td>\n",
       "      <td>16000</td>\n",
       "      <td>346</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.9548)</td>\n",
       "      <td>16000</td>\n",
       "      <td>259</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           recall  sample_size  df_size  \\\n",
       "0  tensor(0.8035)        43136    21550   \n",
       "1  tensor(0.7696)        32384    16162   \n",
       "2  tensor(0.9937)        16000      346   \n",
       "3  tensor(0.9548)        16000      259   \n",
       "\n",
       "                                                disc  \\\n",
       "0  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "1  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "2  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "3  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "\n",
       "                                                 gen df_type  \n",
       "0                                               None     all  \n",
       "1                                               None   train  \n",
       "2  Generator(\\n  (model): Sequential(\\n    (0): L...     all  \n",
       "3  Generator(\\n  (model): Sequential(\\n    (0): L...   train  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result_lst, columns = [\"recall\", \"sample_size\", \"df_size\", \"disc\", \"gen\", \"df_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_lst[3][3]#(t_test.active.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9587)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = dg.fraud.train.df.index\n",
    "# t_test = CustomPandasTorch(dg.all.active.df.query(\"index not in @train_idx\"))\n",
    "t_test = CustomPandasTorch(dg.all.raw.df.query(\"index not in @train_idx\"))\n",
    "recall_calc(result_lst[3][3](t_test.active.data), t_test.active.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9655)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Recall Calculation with best Discriminator (i.e., results_lst[3][3])\n",
    "\n",
    "X_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,:-1]\n",
    "y_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,-1:]\n",
    "recall_calc(result_lst[3][3](X_test), y_test)\n",
    "# display(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = train_loader.dataset.transform_data(*CustomPandasTorch.extractXy(train_loader.dataset.df_test))\n",
    "# (discriminator(X_test).round().flatten() == y_test).to(torch.float32).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = next(iter(train_loader))\n",
    "y_test = y_test.flatten()\n",
    "print((discriminator(X_test).round().flatten() == y_test).to(torch.float32).mean())\n",
    "print('-'*50)\n",
    "y_pred = discriminator(X_test).round().flatten()\n",
    "#Recall Calculation:\n",
    "(((y_pred==1)*(y_test==1)).to(torch.float32).sum())/((y_test==1).to(torch.float32).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_pred.shape, y_test.flatten().shape, discriminator(X_test).round().flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9660, dtype=torch.float64),\n",
       " tensor(0.9690, dtype=torch.float64),\n",
       " tensor(0.9680, dtype=torch.float64),\n",
       " tensor(0.9620, dtype=torch.float64),\n",
       " tensor(0.9645, dtype=torch.float64),\n",
       " tensor(0.9660, dtype=torch.float64),\n",
       " tensor(0.9655, dtype=torch.float64),\n",
       " tensor(0.9620, dtype=torch.float64),\n",
       " tensor(0.9710, dtype=torch.float64),\n",
       " tensor(0.9630, dtype=torch.float64)]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lst = []\n",
    "for _ in range(10):\n",
    "    X, y = train_loader.dataset.sample()\n",
    "    acc_lst.append((discriminator(X).round().flatten() == y).to(torch.float32).mean())\n",
    "acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.8950, dtype=torch.float64),\n",
       " tensor(0.8960, dtype=torch.float64),\n",
       " tensor(0.8945, dtype=torch.float64),\n",
       " tensor(0.9015, dtype=torch.float64),\n",
       " tensor(0.8995, dtype=torch.float64),\n",
       " tensor(0.8975, dtype=torch.float64),\n",
       " tensor(0.8995, dtype=torch.float64),\n",
       " tensor(0.8980, dtype=torch.float64),\n",
       " tensor(0.8995, dtype=torch.float64),\n",
       " tensor(0.8955, dtype=torch.float64)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 0., 0.], grad_fn=<ViewBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Archive Accuracy list:\n",
    "#acc_lst0 = acc_lst\n",
    "# acc_lst1 = [tensor(0.9660, dtype=torch.float64),\n",
    "#  tensor(0.9690, dtype=torch.float64),\n",
    "#  tensor(0.9680, dtype=torch.float64),\n",
    "#  tensor(0.9620, dtype=torch.float64),\n",
    "#  tensor(0.9645, dtype=torch.float64),\n",
    "#  tensor(0.9660, dtype=torch.float64),\n",
    "#  tensor(0.9655, dtype=torch.float64),\n",
    "#  tensor(0.9620, dtype=torch.float64),\n",
    "#  tensor(0.9710, dtype=torch.float64),\n",
    "#  tensor(0.9630, dtype=torch.float64)] \n",
    "#  NOTE: post-resample for training data\n",
    "\n",
    "#acc_test2 = tensor(0.9830)\n",
    "\n",
    "#Archive Predictions:\n",
    "#y_pred0 = y_pred\n",
    "\n",
    "#Archive Recall:\n",
    "#0: tensor(0.8030, dtype=torch.float64)\n",
    "#1: tensor(0.9370, dtype=torch.float64) NOTE: post-resample for training data\n",
    "#2: tensor(0.8652) NOTE: post-resample and train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9370, dtype=torch.float64)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = discriminator(X).round().flatten()\n",
    "#Recall Calculation:\n",
    "(((y_pred==1)*(y==1)).to(torch.float32).sum())/((y==1).to(torch.float32).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8652)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = discriminator(X_test).round().flatten()\n",
    "#Recall Calculation (Test):\n",
    "(((y_pred==1)*(y_test==1)).to(torch.float32).sum())/((y_test==1).to(torch.float32).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Accuracy, Recall Visual\n",
    "![alt text](precision_acc_recall.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_NYCDSA",
   "language": "python",
   "name": "v_nycdsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
