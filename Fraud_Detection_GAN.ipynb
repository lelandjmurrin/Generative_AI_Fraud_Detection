{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GAN Model:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set global PyTorch seed\n",
    "TORCH_SEED = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    21204\n",
       "1      346\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset and drop duplicates\n",
    "fraud_data = pd.read_csv(\"fraud_data.csv\")\n",
    "fraud_data.drop_duplicates(inplace=True)\n",
    "fraud_data.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>50%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>-0.036547</td>\n",
       "      <td>2.092170</td>\n",
       "      <td>-41.928738</td>\n",
       "      <td>-0.007161</td>\n",
       "      <td>2.451888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.044165</td>\n",
       "      <td>1.681791</td>\n",
       "      <td>-40.803981</td>\n",
       "      <td>0.076209</td>\n",
       "      <td>21.467203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>-0.079662</td>\n",
       "      <td>1.850424</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>4.069865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.056684</td>\n",
       "      <td>1.538158</td>\n",
       "      <td>-4.848504</td>\n",
       "      <td>-0.013552</td>\n",
       "      <td>12.114672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>-0.037196</td>\n",
       "      <td>1.521778</td>\n",
       "      <td>-32.092129</td>\n",
       "      <td>-0.066487</td>\n",
       "      <td>29.162172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-0.030833</td>\n",
       "      <td>1.330158</td>\n",
       "      <td>-20.367836</td>\n",
       "      <td>-0.282499</td>\n",
       "      <td>21.393069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>-0.066622</td>\n",
       "      <td>1.562575</td>\n",
       "      <td>-41.506796</td>\n",
       "      <td>0.032245</td>\n",
       "      <td>34.303177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>0.006927</td>\n",
       "      <td>1.318763</td>\n",
       "      <td>-38.987263</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>20.007208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>-0.044095</td>\n",
       "      <td>1.156803</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-0.074955</td>\n",
       "      <td>9.125535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>-0.086981</td>\n",
       "      <td>1.340121</td>\n",
       "      <td>-24.403185</td>\n",
       "      <td>-0.099292</td>\n",
       "      <td>12.701539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.063584</td>\n",
       "      <td>1.148566</td>\n",
       "      <td>-3.995739</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>12.018913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>-0.091154</td>\n",
       "      <td>1.354433</td>\n",
       "      <td>-18.553697</td>\n",
       "      <td>0.127658</td>\n",
       "      <td>3.966626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.991428</td>\n",
       "      <td>-3.844974</td>\n",
       "      <td>-0.016599</td>\n",
       "      <td>4.099352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.086809</td>\n",
       "      <td>1.338749</td>\n",
       "      <td>-19.214325</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>6.441021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>-0.007992</td>\n",
       "      <td>0.916688</td>\n",
       "      <td>-4.498945</td>\n",
       "      <td>0.045088</td>\n",
       "      <td>5.720479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>-0.053081</td>\n",
       "      <td>1.088563</td>\n",
       "      <td>-14.129855</td>\n",
       "      <td>0.059982</td>\n",
       "      <td>6.442798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>-0.094628</td>\n",
       "      <td>1.406832</td>\n",
       "      <td>-24.019099</td>\n",
       "      <td>-0.075292</td>\n",
       "      <td>6.609366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>-0.031292</td>\n",
       "      <td>0.934308</td>\n",
       "      <td>-9.498746</td>\n",
       "      <td>-0.017056</td>\n",
       "      <td>3.790316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>0.022811</td>\n",
       "      <td>0.844682</td>\n",
       "      <td>-4.395283</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>4.851255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>-0.000712</td>\n",
       "      <td>0.728131</td>\n",
       "      <td>-21.024817</td>\n",
       "      <td>-0.056594</td>\n",
       "      <td>13.119819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.765257</td>\n",
       "      <td>-21.453736</td>\n",
       "      <td>-0.023630</td>\n",
       "      <td>27.202839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.731507</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>8.361985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>-0.002914</td>\n",
       "      <td>0.627641</td>\n",
       "      <td>-21.303666</td>\n",
       "      <td>-0.012758</td>\n",
       "      <td>15.626067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.599227</td>\n",
       "      <td>-2.766638</td>\n",
       "      <td>0.035418</td>\n",
       "      <td>4.014444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>-0.002034</td>\n",
       "      <td>0.521377</td>\n",
       "      <td>-4.541819</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>5.541598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.478544</td>\n",
       "      <td>-1.855355</td>\n",
       "      <td>-0.044950</td>\n",
       "      <td>3.463246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.421338</td>\n",
       "      <td>-7.764147</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>9.879903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.302505</td>\n",
       "      <td>-6.520075</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>9.876371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>87.203073</td>\n",
       "      <td>236.275842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.040000</td>\n",
       "      <td>7712.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>0.016056</td>\n",
       "      <td>0.125693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean         std        min        50%          max\n",
       "V1      -0.036547    2.092170 -41.928738  -0.007161     2.451888\n",
       "V2       0.044165    1.681791 -40.803981   0.076209    21.467203\n",
       "V3      -0.079662    1.850424 -31.103685   0.182884     4.069865\n",
       "V4       0.056684    1.538158  -4.848504  -0.013552    12.114672\n",
       "V5      -0.037196    1.521778 -32.092129  -0.066487    29.162172\n",
       "V6      -0.030833    1.330158 -20.367836  -0.282499    21.393069\n",
       "V7      -0.066622    1.562575 -41.506796   0.032245    34.303177\n",
       "V8       0.006927    1.318763 -38.987263   0.022768    20.007208\n",
       "V9      -0.044095    1.156803 -13.434066  -0.074955     9.125535\n",
       "V10     -0.086981    1.340121 -24.403185  -0.099292    12.701539\n",
       "V11      0.063584    1.148566  -3.995739   0.005672    12.018913\n",
       "V12     -0.091154    1.354433 -18.553697   0.127658     3.966626\n",
       "V13      0.000078    0.991428  -3.844974  -0.016599     4.099352\n",
       "V14     -0.086809    1.338749 -19.214325   0.044793     6.441021\n",
       "V15     -0.007992    0.916688  -4.498945   0.045088     5.720479\n",
       "V16     -0.053081    1.088563 -14.129855   0.059982     6.442798\n",
       "V17     -0.094628    1.406832 -24.019099  -0.075292     6.609366\n",
       "V18     -0.031292    0.934308  -9.498746  -0.017056     3.790316\n",
       "V19      0.022811    0.844682  -4.395283   0.022171     4.851255\n",
       "V20     -0.000712    0.728131 -21.024817  -0.056594    13.119819\n",
       "V21      0.008076    0.765257 -21.453736  -0.023630    27.202839\n",
       "V22      0.006185    0.731507  -8.887017   0.008826     8.361985\n",
       "V23     -0.002914    0.627641 -21.303666  -0.012758    15.626067\n",
       "V24     -0.004624    0.599227  -2.766638   0.035418     4.014444\n",
       "V25     -0.002034    0.521377  -4.541819   0.009352     5.541598\n",
       "V26      0.002203    0.478544  -1.855355  -0.044950     3.463246\n",
       "V27      0.001546    0.421338  -7.764147   0.002460     9.879903\n",
       "V28      0.003097    0.302505  -6.520075   0.011821     9.876371\n",
       "Amount  87.203073  236.275842   0.000000  22.040000  7712.430000\n",
       "Class    0.016056    0.125693   0.000000   0.000000     1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display feature statistics\n",
    "fraud_data.describe().T[[\"mean\", \"std\", \"min\", \"50%\", \"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a namedtuple to conveniently hold processed data components\n",
    "PdToTensor = namedtuple(\"PdToTensor\", [\"df\", \"sample\", \"data\", \"labels\"])\n",
    "\n",
    "class CustomPandasTorch(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading and managing tabular data from a pandas DataFrame.\n",
    "\n",
    "    This dataset:\n",
    "    - Takes a pandas DataFrame.\n",
    "    - Splits it into raw, train, and test versions with upsampling to balance classes.\n",
    "    - Provides easy switching between raw, train, and test splits.\n",
    "    - Prepares tensors for PyTorch models (separates features and labels).\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame where the last column is assumed to be the label.\n",
    "\n",
    "    Attributes:\n",
    "        raw (PdToTensor): Full dataset transformed to tensors.\n",
    "        train (PdToTensor): Training dataset (75% split with stratified sampling).\n",
    "        test (PdToTensor): Testing dataset (25% split with stratified sampling).\n",
    "        active (PdToTensor): Currently active dataset (used for loading via __getitem__).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # Transform the entire dataframe into tensors\n",
    "        self.raw = CustomPandasTorch.transform_data(df)\n",
    "        # Set the initial active dataset to raw\n",
    "        self.active = self.raw\n",
    "\n",
    "        # Split the dataframe into train and test sets with stratification on 'Class'\n",
    "        df_train, df_test = train_test_split(df, test_size=0.25, stratify=df.Class, random_state=6)\n",
    "\n",
    "        # Transform both train and test sets into tensors\n",
    "        self.train = CustomPandasTorch.transform_data(df_train)\n",
    "        self.test = CustomPandasTorch.transform_data(df_test)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_data(df_input):\n",
    "        \"\"\"\n",
    "        Transform a DataFrame into tensors, upsampling each class to balance the dataset.\n",
    "\n",
    "        Args:\n",
    "            df_input (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            PdToTensor: A named tuple containing the original dataframe, the upsampled dataframe,\n",
    "                        feature tensors (data), and label tensors (labels).\n",
    "        \"\"\"\n",
    "        # Compute upsampled sample size (next multiple of batch size or at least 16000)\n",
    "        sample_size = max((df_input.shape[0] // BATCH_SIZE + 1) * BATCH_SIZE, 16000)\n",
    "\n",
    "        # Upsample each class separately and shuffle the result\n",
    "        df = (df_input\n",
    "                .groupby('Class')[df_input.columns]\n",
    "                .apply(lambda x: x.sample(sample_size, random_state=6, replace=True))\n",
    "                .sample(frac=1, random_state=6)  # Shuffle the full dataset\n",
    "                .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Convert the dataframe to a float32 tensor for PyTorch compatability\n",
    "        t = torch.from_numpy(df.astype('float32').values)\n",
    "\n",
    "        # Return the dataframe, upsampled dataframe, features (all but last column), and labels (last column)\n",
    "        return PdToTensor(df_input, df, t[:, :-1], t[:, -1:])\n",
    "\n",
    "    def set_raw(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the raw (full) data.\n",
    "        \"\"\"\n",
    "        self.active = self.raw\n",
    "\n",
    "    def set_train(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the training split.\n",
    "        \"\"\"\n",
    "        self.active = self.train\n",
    "\n",
    "    def set_test(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the testing split.\n",
    "        \"\"\"\n",
    "        self.active = self.test\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the currently active dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples.\n",
    "        \"\"\"\n",
    "        return self.active.sample.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a feature-label pair from the active dataset by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (features tensor, label tensor)\n",
    "        \"\"\"\n",
    "        return self.active.data[idx], self.active.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator class: tries to distinguish between real and fake (generated) samples\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A simple feedforward neural network that outputs a probability (real or fake)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(29, 256),    # Input layer (29 features) → hidden layer with 256 units\n",
    "            nn.ReLU(),             # Activation function\n",
    "            nn.Dropout(0.3),        # Dropout for regularization\n",
    "            nn.Linear(256, 64),     # Hidden layer (256 → 64 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),      # Hidden layer (64 → 16 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1),       # Output layer (single unit for binary classification)\n",
    "            nn.Sigmoid(),           # Sigmoid activation to output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass through the discriminator\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Generator class: tries to generate synthetic (fake) samples that resemble real data\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A simple feedforward neural network that maps random noise to synthetic data\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),    # Input: random noise vector (100 dimensions) → 256 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),    # Hidden layer (256 → 128 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),     # Hidden layer (128 → 64 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 29),      # Output: fake sample with 29 features (matching real data)\n",
    "            nn.Tanh(),              # Tanh activation to output values between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass through the generator\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(train_loader):\n",
    "    \"\"\"\n",
    "    Trains a Discriminator neural network using binary cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): A PyTorch DataLoader providing batches of (real_samples, real_labels),\n",
    "                                   where real_samples are input data and real_labels are corresponding labels\n",
    "                                   (typically 0 or 1 for binary classification).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_discriminator, None) — returns the trained discriminator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set initial learning rate\n",
    "    lr = 0.0001\n",
    "\n",
    "    # Define number of epochs for training\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Define loss function: Binary Cross Entropy, commonly used for binary classification tasks\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    # Instantiate the Discriminator model\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # Define optimizer: Adam optimizer used for updating discriminator weights\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over each batch from the training DataLoader\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # Zero the gradients accumulated in the discriminator from previous steps\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the discriminator's predictions on real samples\n",
    "            predictions = discriminator(real_samples)\n",
    "\n",
    "            # Compute the loss between the predictions and true labels\n",
    "            loss_discriminator = loss_function(predictions, real_labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss_discriminator.backward()\n",
    "\n",
    "            # Update the discriminator's parameters based on computed gradients\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # Print the loss once per epoch, at the last batch (where n == BATCH_SIZE - 1)\n",
    "            # if n == BATCH_SIZE - 1:\n",
    "            #     print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "\n",
    "    # Return the trained discriminator model; the second return value is None to sync with train_gan function output\n",
    "    return discriminator, None\n",
    "\n",
    "def train_gan(train_loader):\n",
    "    \"\"\"\n",
    "    Trains a simple Generative Adversarial Network (GAN) composed of a Generator and Discriminator.\n",
    "\n",
    "    The training alternates between:\n",
    "    - Training the Discriminator to distinguish real samples from fake (generated) samples.\n",
    "    - Training the Generator to produce samples that can \"fool\" the Discriminator.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): A PyTorch DataLoader providing batches of (real_samples, real_labels),\n",
    "                                   where real_samples are true data examples and real_labels are the corresponding labels (typically ones).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_discriminator, trained_generator) — the trained Discriminator and Generator models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set initial learning rate\n",
    "    lr = 0.0001\n",
    "\n",
    "    # Define number of epochs for training\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Define the loss function: Binary Cross Entropy (BCE) Loss, used for binary classification tasks\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    # Instantiate Discriminator and Generator models\n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    \n",
    "    # Define optimizers for both discriminator and generator using Adam optimizer\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over each batch from the training DataLoader\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # ------------------------\n",
    "            # Train the Discriminator\n",
    "            # ------------------------\n",
    "\n",
    "            # Generate fake samples using random noise input\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "\n",
    "            # Create fake labels (zeros) for generated samples\n",
    "            gen_labels = torch.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "            # Combine real and generated samples\n",
    "            all_samples = torch.cat((real_samples, gen_samples))\n",
    "\n",
    "            # Combine real and fake labels\n",
    "            all_labels = torch.cat((real_labels, gen_labels))\n",
    "\n",
    "            # Zero the discriminator's gradient buffers\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Compute discriminator loss on both real and fake samples\n",
    "            loss_discriminator = loss_function(discriminator(all_samples), all_labels)\n",
    "\n",
    "            # Backpropagation and optimizer step for discriminator\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # ------------------------\n",
    "            # Train the Generator\n",
    "            # ------------------------\n",
    "\n",
    "            # Zero the generator's gradient buffers\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # Generate new fake samples for generator training\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "\n",
    "            # Attempt to \"fool\" the discriminator: generator wants discriminator to predict real labels (ones)\n",
    "            loss_generator = loss_function(discriminator(gen_samples), real_labels)\n",
    "\n",
    "            # Backpropagation and optimizer step for generator\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # ------------------------\n",
    "            # Logging\n",
    "            # ------------------------\n",
    "\n",
    "            # Print loss at the last batch of the epoch\n",
    "            # if n == BATCH_SIZE - 1:\n",
    "            #     print(f\"Epoch: {epoch} Loss D.: {loss_discriminator.item()}\")\n",
    "            #     print(f\"Epoch: {epoch} Loss G.: {loss_generator.item()}\")\n",
    "\n",
    "    # Return both trained models\n",
    "    return discriminator, generator\n",
    "\n",
    "def recall_calc(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Calculates Recall for binary classification.\n",
    "    Note: this is preferred over accuracy for predicting fraudulent credit card transactions because:\n",
    "                1. There's so few fraudulent record, accuracy can be high even if it incorrectly returns all negatives\n",
    "                2. False negatives are far higher priority than false positives (i.e., more repercussions when incorrectly predicting fraudulent records to be valid)\n",
    "\n",
    "    Recall = (True Positives) / (All Fraudulent Records)\n",
    "\n",
    "    Args:\n",
    "        y_pred (Tensor): Predicted labels (0 for non-fraud, 1 for fraud).\n",
    "        y_test (Tensor): True labels (ground truth; 0 for non-fraud, 1 for fraud).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Recall value as a float tensor (scalar).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate number of true positives:\n",
    "    # - y_pred == 1 identifies predicted positives\n",
    "    # - y_test == 1 identifies fraudulent records\n",
    "    # - Their element-wise multiplication gives True Positives (correctly predicted frauds)\n",
    "    true_positives = ((y_pred == 1) * (y_test == 1)).to(torch.float32).sum()\n",
    "\n",
    "    # Calculate total number of actual fraud cases\n",
    "    actual_fraud = (y_test == 1).to(torch.float32).sum()\n",
    "\n",
    "    # Recall is the ratio of true positives to actual fraud cases\n",
    "    recall = true_positives / actual_fraud\n",
    "\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(TORCH_SEED)\n",
    "DataGroups = namedtuple(\"DataGroups\", [\"all\", \"fraud\", \"valid\"])\n",
    "dg = DataGroups(CustomPandasTorch(fraud_data), CustomPandasTorch(fraud_data.query(\"Class == 1\")), CustomPandasTorch(fraud_data.query(\"Class == 0\")))\n",
    "\n",
    "#Loop 1:\n",
    "#   records: all (21,550)\n",
    "#   training: discriminator only\n",
    "#Loop 2:\n",
    "#   records: fraudulent only (346)\n",
    "#   training: discriminator+generator (GAN)\n",
    "#Within each loop:\n",
    "#   Recall is calculated for:\n",
    "#        1. all records (loop 1: 21,550 | loop 2: 346)\n",
    "#        2. 25% of the records after 75% (loop 1: 16,162 | loop 2: 259) were used for training \n",
    "\n",
    "#This cell takes ~2.5-4mins to run\n",
    "\n",
    "result_lst = []\n",
    "\n",
    "for i, (t, func) in enumerate(zip([dg.all, dg.fraud], [train_discriminator, train_gan])):\n",
    "    print(i)\n",
    "\n",
    "    #Set active dataset to raw\n",
    "    t.set_raw()\n",
    "\n",
    "    #All records recall score\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True) \n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.active.data), t.active.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"all\"])\n",
    "    \n",
    "    #Set active dataset to train\n",
    "    t.set_train() \n",
    "\n",
    "    #25% test records recall score\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.test.data), t.test.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"train\"])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>df_size</th>\n",
       "      <th>disc</th>\n",
       "      <th>gen</th>\n",
       "      <th>df_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.7940)</td>\n",
       "      <td>43136</td>\n",
       "      <td>21550</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.8040)</td>\n",
       "      <td>32384</td>\n",
       "      <td>16162</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.9944)</td>\n",
       "      <td>16000</td>\n",
       "      <td>346</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.9625)</td>\n",
       "      <td>16000</td>\n",
       "      <td>259</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           recall  sample_size  df_size  \\\n",
       "0  tensor(0.7940)        43136    21550   \n",
       "1  tensor(0.8040)        32384    16162   \n",
       "2  tensor(0.9944)        16000      346   \n",
       "3  tensor(0.9625)        16000      259   \n",
       "\n",
       "                                                disc  \\\n",
       "0  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "1  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "2  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "3  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "\n",
       "                                                 gen df_type  \n",
       "0                                               None     all  \n",
       "1                                               None   train  \n",
       "2  Generator(\\n  (model): Sequential(\\n    (0): L...     all  \n",
       "3  Generator(\\n  (model): Sequential(\\n    (0): L...   train  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(result_lst, columns = [\"recall\", \"sample_size\", \"df_size\", \"disc\", \"gen\", \"df_type\"])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training for best discriminator:\n",
    "#1. 75% of all fraudulent records were used to train (other 25% used later for testing along with 100% of valid records)\n",
    "#2. Used resampling to upsample fraudulent training records to 16,000 (up from 259)\n",
    "#3. Used GAN model to generate extra fraduluent training records for improved recall\n",
    "best_discriminator = df_results[\"disc\"][3]\n",
    "best_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test set recall: 0.954\n"
     ]
    }
   ],
   "source": [
    "#Testing best discriminator:\n",
    "#1. Test set includes 25% of the fraudulent records (not used for training) and all valid records\n",
    "#2. Final Test Data Recall Calculation with best Discriminator below (i.e., best_discriminator)\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "train_idx = dg.fraud.train.df.index\n",
    "X_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,:-1]\n",
    "y_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,-1:]\n",
    "recall_result = recall_calc(best_discriminator(X_test), y_test).item()\n",
    "print(f\"Final test set recall: {recall_result:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further Work TODO:\n",
    "\n",
    "#1. Standardize Amount feature\n",
    "#2. Increase Number of Epochs\n",
    "#3. Analyze ROC and recall-precision curves\n",
    "#4. Hyperparametrization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vPortfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
