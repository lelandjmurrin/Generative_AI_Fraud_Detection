{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# GAN Model:\n",
    "import torch\n",
    "from torch import nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
    "# from tensorflow.keras.layers import BatchNormalization, LeakyReLU\n",
    "# from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set global PyTorch seed\n",
    "TORCH_SEED = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    21204\n",
       "1      346\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "fraud_data = pd.read_csv(\"fraud_data.csv\")\n",
    "fraud_data.drop_duplicates(inplace=True)\n",
    "fraud_data.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_strat = np.hstack([np.ones(shape = 5000), np.zeros(shape = fraud_data.shape[0]-5000)])\n",
    "# df_train, df_test = train_test_split(fraud_data, test_size=0.25, stratify = fraud_data.Class)\n",
    "# display(df_train.Class.value_counts(normalize = True), df_test.Class.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# PdToTensor = namedtuple(\"PdToTensor\",[\"df\",\"sample\",\"data\",\"labels\"])\n",
    "\n",
    "# class CustomPandasTorch(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.raw = CustomPandasTorch.transform_data(df)\n",
    "#         self.active = self.raw\n",
    "        \n",
    "#         df_train, df_test = train_test_split(df, test_size = 0.25, stratify = df.Class, random_state = 6)\n",
    "#         self.train, self.test = CustomPandasTorch.transform_data(df_train), CustomPandasTorch.transform_data(df_test)\n",
    "\n",
    "\n",
    "#     def transform_data(df_input):\n",
    "#         sample_size = max((df_input.shape[0]//BATCH_SIZE+1)*BATCH_SIZE, 16000)\n",
    "#         df = (df_input\n",
    "#                 .groupby('Class')\n",
    "#                 [df_input.columns]\n",
    "#                 .apply(lambda x: x.sample(sample_size, random_state = 6, replace = True))\n",
    "#                 .sample(frac=1, random_state = 6)\n",
    "#                 .reset_index(drop = True)        \n",
    "#         )\n",
    "\n",
    "#         t = torch.from_numpy(df.astype('float32').values)\n",
    "#         return PdToTensor(df_input, df, t[:,:-1], t[:,-1:])\n",
    "    \n",
    "#     def set_raw(self):\n",
    "#         self.active = self.raw\n",
    "    \n",
    "#     def set_train(self):\n",
    "#         self.active = self.train\n",
    "\n",
    "#     def set_test(self):\n",
    "#         self.active = self.test\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.active.sample.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.active.data[idx], self.active.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a namedtuple to conveniently hold processed data components\n",
    "PdToTensor = namedtuple(\"PdToTensor\", [\"df\", \"sample\", \"data\", \"labels\"])\n",
    "\n",
    "class CustomPandasTorch(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading and managing tabular data from a pandas DataFrame.\n",
    "\n",
    "    This dataset:\n",
    "    - Takes a pandas DataFrame.\n",
    "    - Splits it into raw, train, and test versions with upsampling to balance classes.\n",
    "    - Provides easy switching between raw, train, and test splits.\n",
    "    - Prepares tensors for PyTorch models (separates features and labels).\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame where the last column is assumed to be the label.\n",
    "\n",
    "    Attributes:\n",
    "        raw (PdToTensor): Full dataset transformed to tensors.\n",
    "        train (PdToTensor): Training dataset (75% split with stratified sampling).\n",
    "        test (PdToTensor): Testing dataset (25% split with stratified sampling).\n",
    "        active (PdToTensor): Currently active dataset (used for loading via __getitem__).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # Transform the entire dataframe into tensors\n",
    "        self.raw = CustomPandasTorch.transform_data(df)\n",
    "        # Set the initial active dataset to raw\n",
    "        self.active = self.raw\n",
    "\n",
    "        # Split the dataframe into train and test sets with stratification on 'Class'\n",
    "        df_train, df_test = train_test_split(df, test_size=0.25, stratify=df.Class, random_state=6)\n",
    "\n",
    "        # Transform both train and test sets into tensors\n",
    "        self.train = CustomPandasTorch.transform_data(df_train)\n",
    "        self.test = CustomPandasTorch.transform_data(df_test)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_data(df_input):\n",
    "        \"\"\"\n",
    "        Transform a DataFrame into tensors, upsampling each class to balance the dataset.\n",
    "\n",
    "        Args:\n",
    "            df_input (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            PdToTensor: A named tuple containing the original dataframe, the upsampled dataframe,\n",
    "                        feature tensors (data), and label tensors (labels).\n",
    "        \"\"\"\n",
    "        # Compute upsampled sample size (next multiple of batch size or at least 16000)\n",
    "        sample_size = max((df_input.shape[0] // BATCH_SIZE + 1) * BATCH_SIZE, 16000)\n",
    "\n",
    "        # Upsample each class separately and shuffle the result\n",
    "        df = (df_input\n",
    "                .groupby('Class')[df_input.columns]\n",
    "                .apply(lambda x: x.sample(sample_size, random_state=6, replace=True))\n",
    "                .sample(frac=1, random_state=6)  # Shuffle the full dataset\n",
    "                .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Convert the dataframe to a float32 tensor for PyTorch compatability\n",
    "        t = torch.from_numpy(df.astype('float32').values)\n",
    "\n",
    "        # Return the dataframe, upsampled dataframe, features (all but last column), and labels (last column)\n",
    "        return PdToTensor(df_input, df, t[:, :-1], t[:, -1:])\n",
    "\n",
    "    def set_raw(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the raw (full) data.\n",
    "        \"\"\"\n",
    "        self.active = self.raw\n",
    "\n",
    "    def set_train(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the training split.\n",
    "        \"\"\"\n",
    "        self.active = self.train\n",
    "\n",
    "    def set_test(self):\n",
    "        \"\"\"\n",
    "        Switch the active dataset to the testing split.\n",
    "        \"\"\"\n",
    "        self.active = self.test\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the currently active dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples.\n",
    "        \"\"\"\n",
    "        return self.active.sample.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a feature-label pair from the active dataset by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (features tensor, label tensor)\n",
    "        \"\"\"\n",
    "        return self.active.data[idx], self.active.labels[idx]\n",
    "    \n",
    "# DataGroups = namedtuple(\"DataGroups\", [\"all\", \"fraud\", \"valid\"])\n",
    "# dg = DataGroups(CustomPandasTorch(fraud_data), CustomPandasTorch(fraud_data.query(\"Class == 1\")), CustomPandasTorch(fraud_data.query(\"Class == 0\")))\n",
    "# dg.all.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataGroups = namedtuple(\"DataGroups\", [\"all\", \"fraud\", \"valid\"])\n",
    "# # train_fraud = fraud_data.sample(n = 8000).astype('float32')\n",
    "# # train_fraud = CustomPandasTorch.pd_sample(fraud_data, size = 4000)\n",
    "# # train_fraud_torch = CustomPandasTorch(\n",
    "# #                                         fraud_data.query(\"Class == 1\")#.sample(512, replace = True).reset_index(drop=True)\n",
    "# #                                         # transform = torch.from_numpy,\n",
    "# #                                         # target_transform = lambda x: torch.from_numpy(np.array(x).astype('float32')).unsqueeze(dim=0),\n",
    "# #                                         # transform_type = None\n",
    "# #                                     )\n",
    "\n",
    "# dg = DataGroups(CustomPandasTorch(fraud_data), CustomPandasTorch(fraud_data.query(\"Class == 1\")), CustomPandasTorch(fraud_data.query(\"Class == 0\")))\n",
    "# #TODO: April 2 create 3 train_fraud_torch dataset (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_loader = torch.utils.data.DataLoader(\n",
    "# #     train_fraud_torch, batch_size=BATCH_SIZE, shuffle=True\n",
    "# # )\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dg.fraud, batch_size=BATCH_SIZE, shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader.dataset.set_raw()\n",
    "# data, label = next(iter(train_loader))\n",
    "# display(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(29, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(16, 1),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = self.model(x)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(100, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 29),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = self.model(x)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator class: tries to distinguish between real and fake (generated) samples\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A simple feedforward neural network that outputs a probability (real or fake)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(29, 256),    # Input layer (29 features) → hidden layer with 256 units\n",
    "            nn.ReLU(),             # Activation function\n",
    "            nn.Dropout(0.3),        # Dropout for regularization\n",
    "            nn.Linear(256, 64),     # Hidden layer (256 → 64 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),      # Hidden layer (64 → 16 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1),       # Output layer (single unit for binary classification)\n",
    "            nn.Sigmoid(),           # Sigmoid activation to output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass through the discriminator\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Generator class: tries to generate synthetic (fake) samples that resemble real data\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A simple feedforward neural network that maps random noise to synthetic data\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),    # Input: random noise vector (100 dimensions) → 256 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),    # Hidden layer (256 → 128 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),     # Hidden layer (128 → 64 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 29),      # Output: fake sample with 29 features (matching real data)\n",
    "            nn.Tanh(),              # Tanh activation to output values between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass through the generator\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "# num_epochs = 50\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "# # optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "# # optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def train_discriminator(train_loader):\n",
    "# #     lr = 0.0001\n",
    "# #     num_epochs = 50\n",
    "# #     loss_function = nn.BCELoss()\n",
    "\n",
    "# #     discriminator = Discriminator()\n",
    "    \n",
    "# #     optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# #     for epoch in range(num_epochs):\n",
    "# #         for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "# #             # Training the discriminator\n",
    "# #             discriminator.zero_grad()\n",
    "# #             loss_discriminator = loss_function(discriminator(real_samples), real_labels)\n",
    "# #             loss_discriminator.backward()\n",
    "# #             optimizer_discriminator.step()\n",
    "            \n",
    "# #             # Show loss\n",
    "# #             if n == BATCH_SIZE - 1:\n",
    "# #                 print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "\n",
    "# #     return discriminator, None\n",
    "\n",
    "# def train_gan(train_loader):\n",
    "#     lr = 0.0001\n",
    "#     num_epochs = 50\n",
    "#     loss_function = nn.BCELoss()\n",
    "\n",
    "#     discriminator = Discriminator()\n",
    "#     generator = Generator()\n",
    "    \n",
    "#     optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "#     optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "#             # Data for training discriminator\n",
    "#             gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "#             gen_labels = torch.zeros((BATCH_SIZE, 1))\n",
    "#             all_samples = torch.cat((real_samples, gen_samples))\n",
    "#             all_labels = torch.cat((real_labels, gen_labels))\n",
    "\n",
    "#             # Training the discriminator\n",
    "#             discriminator.zero_grad()\n",
    "#             loss_discriminator = loss_function(discriminator(all_samples), all_labels)\n",
    "#             loss_discriminator.backward()\n",
    "#             optimizer_discriminator.step()\n",
    "            \n",
    "#             # Data and training the generator\n",
    "#             generator.zero_grad()\n",
    "#             gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "#             loss_generator = loss_function(discriminator(gen_samples), real_labels) \n",
    "#             loss_generator.backward()\n",
    "#             optimizer_generator.step()\n",
    "\n",
    "#             # Show loss\n",
    "#             if n == BATCH_SIZE - 1:\n",
    "#                 print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "#                 print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
    "\n",
    "#     return discriminator, generator\n",
    "\n",
    "\n",
    "# #Recall Calc:\n",
    "# def recall_calc(y_pred, y_test):\n",
    "#     return (((y_pred==1)*(y_test==1)).to(torch.float32).sum())/((y_test==1).to(torch.float32).sum())\n",
    "\n",
    "# # discriminator, generator = train_gan(train_loader)\n",
    "# # recall_calc(discriminator(train_loader.dataset.active.data), train_loader.dataset.active.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(train_loader):\n",
    "    \"\"\"\n",
    "    Trains a Discriminator neural network using binary cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): A PyTorch DataLoader providing batches of (real_samples, real_labels),\n",
    "                                   where real_samples are input data and real_labels are corresponding labels\n",
    "                                   (typically 0 or 1 for binary classification).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_discriminator, None) — returns the trained discriminator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set initial learning rate\n",
    "    lr = 0.0001\n",
    "\n",
    "    # Define number of epochs for training\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Define loss function: Binary Cross Entropy, commonly used for binary classification tasks\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    # Instantiate the Discriminator model\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # Define optimizer: Adam optimizer used for updating discriminator weights\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over each batch from the training DataLoader\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # Zero the gradients accumulated in the discriminator from previous steps\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the discriminator's predictions on real samples\n",
    "            predictions = discriminator(real_samples)\n",
    "\n",
    "            # Compute the loss between the predictions and true labels\n",
    "            loss_discriminator = loss_function(predictions, real_labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            loss_discriminator.backward()\n",
    "\n",
    "            # Update the discriminator's parameters based on computed gradients\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # Print the loss once per epoch, at the last batch (where n == BATCH_SIZE - 1)\n",
    "            if n == BATCH_SIZE - 1:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "\n",
    "    # Return the trained discriminator model; the second return value is None to sync with train_gan function output\n",
    "    return discriminator, None\n",
    "\n",
    "def train_gan(train_loader):\n",
    "    \"\"\"\n",
    "    Trains a simple Generative Adversarial Network (GAN) composed of a Generator and Discriminator.\n",
    "\n",
    "    The training alternates between:\n",
    "    - Training the Discriminator to distinguish real samples from fake (generated) samples.\n",
    "    - Training the Generator to produce samples that can \"fool\" the Discriminator.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): A PyTorch DataLoader providing batches of (real_samples, real_labels),\n",
    "                                   where real_samples are true data examples and real_labels are the corresponding labels (typically ones).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_discriminator, trained_generator) — the trained Discriminator and Generator models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set initial learning rate\n",
    "    lr = 0.0001\n",
    "\n",
    "    # Define number of epochs for training\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Define the loss function: Binary Cross Entropy (BCE) Loss, used for binary classification tasks\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    # Instantiate Discriminator and Generator models\n",
    "    discriminator = Discriminator()\n",
    "    generator = Generator()\n",
    "    \n",
    "    # Define optimizers for both discriminator and generator using Adam optimizer\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over each batch from the training DataLoader\n",
    "        for n, (real_samples, real_labels) in enumerate(train_loader):\n",
    "            # ------------------------\n",
    "            # Train the Discriminator\n",
    "            # ------------------------\n",
    "\n",
    "            # Generate fake samples using random noise input\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "\n",
    "            # Create fake labels (zeros) for generated samples\n",
    "            gen_labels = torch.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "            # Combine real and generated samples\n",
    "            all_samples = torch.cat((real_samples, gen_samples))\n",
    "\n",
    "            # Combine real and fake labels\n",
    "            all_labels = torch.cat((real_labels, gen_labels))\n",
    "\n",
    "            # Zero the discriminator's gradient buffers\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Compute discriminator loss on both real and fake samples\n",
    "            loss_discriminator = loss_function(discriminator(all_samples), all_labels)\n",
    "\n",
    "            # Backpropagation and optimizer step for discriminator\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "            \n",
    "            # ------------------------\n",
    "            # Train the Generator\n",
    "            # ------------------------\n",
    "\n",
    "            # Zero the generator's gradient buffers\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # Generate new fake samples for generator training\n",
    "            gen_samples = generator(torch.randn((BATCH_SIZE, 100)))\n",
    "\n",
    "            # Attempt to \"fool\" the discriminator: generator wants discriminator to predict real labels (ones)\n",
    "            loss_generator = loss_function(discriminator(gen_samples), real_labels)\n",
    "\n",
    "            # Backpropagation and optimizer step for generator\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # ------------------------\n",
    "            # Logging\n",
    "            # ------------------------\n",
    "\n",
    "            # Print loss at the last batch of the epoch\n",
    "            if n == BATCH_SIZE - 1:\n",
    "                print(f\"Epoch: {epoch} Loss D.: {loss_discriminator.item()}\")\n",
    "                print(f\"Epoch: {epoch} Loss G.: {loss_generator.item()}\")\n",
    "\n",
    "    # Return both trained models\n",
    "    return discriminator, generator\n",
    "\n",
    "def recall_calc(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Calculates Recall for binary classification.\n",
    "    Note: this is preferred over accuracy for predicting fraudulent credit card transactions because:\n",
    "                1. There's so few fraudulent record, accuracy can be high even if it incorrectly returns all negatives\n",
    "                2. False negatives are far higher priority than false positives (i.e., more repercussions when incorrectly predicting fraudulent records to be valid)\n",
    "\n",
    "    Recall = (True Positives) / (All Fraudulent Records)\n",
    "\n",
    "    Args:\n",
    "        y_pred (Tensor): Predicted labels (0 for non-fraud, 1 for fraud).\n",
    "        y_test (Tensor): True labels (ground truth; 0 for non-fraud, 1 for fraud).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Recall value as a float tensor (scalar).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate number of true positives:\n",
    "    # - y_pred == 1 identifies predicted positives\n",
    "    # - y_test == 1 identifies fraudulent records\n",
    "    # - Their element-wise multiplication gives True Positives (correctly predicted frauds)\n",
    "    true_positives = ((y_pred == 1) * (y_test == 1)).to(torch.float32).sum()\n",
    "\n",
    "    # Calculate total number of actual fraud cases\n",
    "    actual_fraud = (y_test == 1).to(torch.float32).sum()\n",
    "\n",
    "    # Recall is the ratio of true positives to actual fraud cases\n",
    "    recall = true_positives / actual_fraud\n",
    "\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0 Loss D.: 0.6411410570144653\n",
      "Epoch: 1 Loss D.: 0.25355860590934753\n",
      "Epoch: 2 Loss D.: 0.138943150639534\n",
      "Epoch: 3 Loss D.: 0.01684289053082466\n",
      "Epoch: 4 Loss D.: 0.06362665444612503\n",
      "Epoch: 5 Loss D.: 0.08037815243005753\n",
      "Epoch: 6 Loss D.: 0.09429124742746353\n",
      "Epoch: 7 Loss D.: 0.057613976299762726\n",
      "Epoch: 8 Loss D.: 0.02069392427802086\n",
      "Epoch: 9 Loss D.: 0.013533125631511211\n",
      "Epoch: 10 Loss D.: 0.009376178495585918\n",
      "Epoch: 11 Loss D.: 0.03439208120107651\n",
      "Epoch: 12 Loss D.: 0.011033596470952034\n",
      "Epoch: 13 Loss D.: 0.13157817721366882\n",
      "Epoch: 14 Loss D.: 0.24584457278251648\n",
      "Epoch: 15 Loss D.: 0.08837386965751648\n",
      "Epoch: 16 Loss D.: 0.040970440953969955\n",
      "Epoch: 17 Loss D.: 0.08025602251291275\n",
      "Epoch: 18 Loss D.: 0.10956155508756638\n",
      "Epoch: 19 Loss D.: 0.0227934829890728\n",
      "Epoch: 20 Loss D.: 0.10689669102430344\n",
      "Epoch: 21 Loss D.: 0.05871668457984924\n",
      "Epoch: 22 Loss D.: 0.0015538762090727687\n",
      "Epoch: 23 Loss D.: 0.005364687647670507\n",
      "Epoch: 24 Loss D.: 0.006842965260148048\n",
      "Epoch: 25 Loss D.: 0.028119806200265884\n",
      "Epoch: 26 Loss D.: 0.25092950463294983\n",
      "Epoch: 27 Loss D.: 0.002528855111449957\n",
      "Epoch: 28 Loss D.: 0.00485282763838768\n",
      "Epoch: 29 Loss D.: 0.001410854165442288\n",
      "Epoch: 30 Loss D.: 0.06629111617803574\n",
      "Epoch: 31 Loss D.: 0.21960654854774475\n",
      "Epoch: 32 Loss D.: 0.0008586153853684664\n",
      "Epoch: 33 Loss D.: 0.0030135037377476692\n",
      "Epoch: 34 Loss D.: 0.004059699829667807\n",
      "Epoch: 35 Loss D.: 8.824745236779563e-06\n",
      "Epoch: 36 Loss D.: 0.00785827450454235\n",
      "Epoch: 37 Loss D.: 0.0032732593826949596\n",
      "Epoch: 38 Loss D.: 0.10951349884271622\n",
      "Epoch: 39 Loss D.: 7.772911339998245e-05\n",
      "Epoch: 40 Loss D.: 0.0006108772358857095\n",
      "Epoch: 41 Loss D.: 0.0004983618855476379\n",
      "Epoch: 42 Loss D.: 0.0033784154802560806\n",
      "Epoch: 43 Loss D.: 0.024431753903627396\n",
      "Epoch: 44 Loss D.: 4.880510095972568e-05\n",
      "Epoch: 45 Loss D.: 0.011693133972585201\n",
      "Epoch: 46 Loss D.: 0.021648626774549484\n",
      "Epoch: 47 Loss D.: 0.00015756202628836036\n",
      "Epoch: 48 Loss D.: 0.0007447474054060876\n",
      "Epoch: 49 Loss D.: 0.039539139717817307\n",
      "Epoch: 0 Loss D.: 0.6792786717414856\n",
      "Epoch: 1 Loss D.: 0.30710265040397644\n",
      "Epoch: 2 Loss D.: 0.17698422074317932\n",
      "Epoch: 3 Loss D.: 0.4027981162071228\n",
      "Epoch: 4 Loss D.: 0.1623719334602356\n",
      "Epoch: 5 Loss D.: 0.10022443532943726\n",
      "Epoch: 6 Loss D.: 0.0998576208949089\n",
      "Epoch: 7 Loss D.: 0.09040264040231705\n",
      "Epoch: 8 Loss D.: 0.08580394834280014\n",
      "Epoch: 9 Loss D.: 0.16795195639133453\n",
      "Epoch: 10 Loss D.: 0.030307842418551445\n",
      "Epoch: 11 Loss D.: 0.07758477330207825\n",
      "Epoch: 12 Loss D.: 0.032523464411497116\n",
      "Epoch: 13 Loss D.: 0.15371635556221008\n",
      "Epoch: 14 Loss D.: 0.17388983070850372\n",
      "Epoch: 15 Loss D.: 0.0024112937971949577\n",
      "Epoch: 16 Loss D.: 0.06620299071073532\n",
      "Epoch: 17 Loss D.: 0.05979030579328537\n",
      "Epoch: 18 Loss D.: 0.016804978251457214\n",
      "Epoch: 19 Loss D.: 0.03773513436317444\n",
      "Epoch: 20 Loss D.: 0.032888200134038925\n",
      "Epoch: 21 Loss D.: 0.05893182009458542\n",
      "Epoch: 22 Loss D.: 0.06575016677379608\n",
      "Epoch: 23 Loss D.: 0.10781214386224747\n",
      "Epoch: 24 Loss D.: 0.03684351593255997\n",
      "Epoch: 25 Loss D.: 0.1531379669904709\n",
      "Epoch: 26 Loss D.: 0.05062302574515343\n",
      "Epoch: 27 Loss D.: 0.09589794278144836\n",
      "Epoch: 28 Loss D.: 0.027744023129343987\n",
      "Epoch: 29 Loss D.: 0.0006949878297746181\n",
      "Epoch: 30 Loss D.: 0.016675062477588654\n",
      "Epoch: 31 Loss D.: 0.00220114435069263\n",
      "Epoch: 32 Loss D.: 0.044883642345666885\n",
      "Epoch: 33 Loss D.: 0.0013627681182697415\n",
      "Epoch: 34 Loss D.: 0.030826719477772713\n",
      "Epoch: 35 Loss D.: 0.0006318562664091587\n",
      "Epoch: 36 Loss D.: 0.013174002058804035\n",
      "Epoch: 37 Loss D.: 0.058122944086790085\n",
      "Epoch: 38 Loss D.: 0.03609168902039528\n",
      "Epoch: 39 Loss D.: 0.002743431832641363\n",
      "Epoch: 40 Loss D.: 0.00017581151041667908\n",
      "Epoch: 41 Loss D.: 0.00303919380530715\n",
      "Epoch: 42 Loss D.: 0.018919242545962334\n",
      "Epoch: 43 Loss D.: 0.014869607985019684\n",
      "Epoch: 44 Loss D.: 0.08422407507896423\n",
      "Epoch: 45 Loss D.: 3.2183408737182617\n",
      "Epoch: 46 Loss D.: 0.014571677893400192\n",
      "Epoch: 47 Loss D.: 0.0009842005092650652\n",
      "Epoch: 48 Loss D.: 0.05382870137691498\n",
      "Epoch: 49 Loss D.: 0.009057319723069668\n",
      "1\n",
      "Epoch: 0 Loss D.: 0.47973328828811646\n",
      "Epoch: 0 Loss G.: 0.661569356918335\n",
      "Epoch: 1 Loss D.: 0.2854427099227905\n",
      "Epoch: 1 Loss G.: 0.9964419007301331\n",
      "Epoch: 2 Loss D.: 0.1124492883682251\n",
      "Epoch: 2 Loss G.: 1.992816686630249\n",
      "Epoch: 3 Loss D.: 0.06974324584007263\n",
      "Epoch: 3 Loss G.: 2.8186745643615723\n",
      "Epoch: 4 Loss D.: 0.1371157169342041\n",
      "Epoch: 4 Loss G.: 3.313298463821411\n",
      "Epoch: 5 Loss D.: 0.03566094860434532\n",
      "Epoch: 5 Loss G.: 3.895125389099121\n",
      "Epoch: 6 Loss D.: 0.046439845114946365\n",
      "Epoch: 6 Loss G.: 5.0455827713012695\n",
      "Epoch: 7 Loss D.: 0.020766038447618484\n",
      "Epoch: 7 Loss G.: 5.086135387420654\n",
      "Epoch: 8 Loss D.: 0.038577403873205185\n",
      "Epoch: 8 Loss G.: 5.057692527770996\n",
      "Epoch: 9 Loss D.: 0.008383734151721\n",
      "Epoch: 9 Loss G.: 5.987836837768555\n",
      "Epoch: 10 Loss D.: 0.05209501087665558\n",
      "Epoch: 10 Loss G.: 4.84083890914917\n",
      "Epoch: 11 Loss D.: 0.06504231691360474\n",
      "Epoch: 11 Loss G.: 6.2570648193359375\n",
      "Epoch: 12 Loss D.: 0.020483341068029404\n",
      "Epoch: 12 Loss G.: 5.4534406661987305\n",
      "Epoch: 13 Loss D.: 0.002683396451175213\n",
      "Epoch: 13 Loss G.: 5.901973247528076\n",
      "Epoch: 14 Loss D.: 0.006689052097499371\n",
      "Epoch: 14 Loss G.: 5.701548099517822\n",
      "Epoch: 15 Loss D.: 0.0035333933774381876\n",
      "Epoch: 15 Loss G.: 7.39109992980957\n",
      "Epoch: 16 Loss D.: 0.02325032837688923\n",
      "Epoch: 16 Loss G.: 6.599087715148926\n",
      "Epoch: 17 Loss D.: 0.005139408633112907\n",
      "Epoch: 17 Loss G.: 7.497957229614258\n",
      "Epoch: 18 Loss D.: 0.0016400094609707594\n",
      "Epoch: 18 Loss G.: 7.596184730529785\n",
      "Epoch: 19 Loss D.: 0.017605798318982124\n",
      "Epoch: 19 Loss G.: 5.538910865783691\n",
      "Epoch: 20 Loss D.: 0.01116833183914423\n",
      "Epoch: 20 Loss G.: 7.600691318511963\n",
      "Epoch: 21 Loss D.: 0.0020506754517555237\n",
      "Epoch: 21 Loss G.: 7.051650047302246\n",
      "Epoch: 22 Loss D.: 0.0014586313627660275\n",
      "Epoch: 22 Loss G.: 7.66248083114624\n",
      "Epoch: 23 Loss D.: 0.0010119666112586856\n",
      "Epoch: 23 Loss G.: 8.19074821472168\n",
      "Epoch: 24 Loss D.: 0.0021193944849073887\n",
      "Epoch: 24 Loss G.: 6.207888126373291\n",
      "Epoch: 25 Loss D.: 0.00399361364543438\n",
      "Epoch: 25 Loss G.: 6.130380630493164\n",
      "Epoch: 26 Loss D.: 0.003343168180435896\n",
      "Epoch: 26 Loss G.: 8.007118225097656\n",
      "Epoch: 27 Loss D.: 0.0014842792879790068\n",
      "Epoch: 27 Loss G.: 7.758673667907715\n",
      "Epoch: 28 Loss D.: 0.09446614235639572\n",
      "Epoch: 28 Loss G.: 9.317972183227539\n",
      "Epoch: 29 Loss D.: 0.0005165214533917606\n",
      "Epoch: 29 Loss G.: 8.745206832885742\n",
      "Epoch: 30 Loss D.: 7.462904613930732e-05\n",
      "Epoch: 30 Loss G.: 12.552732467651367\n",
      "Epoch: 31 Loss D.: 0.008760521188378334\n",
      "Epoch: 31 Loss G.: 10.591778755187988\n",
      "Epoch: 32 Loss D.: 0.0038935691118240356\n",
      "Epoch: 32 Loss G.: 8.61165714263916\n",
      "Epoch: 33 Loss D.: 0.02083408087491989\n",
      "Epoch: 33 Loss G.: 8.52685832977295\n",
      "Epoch: 34 Loss D.: 0.002103344304487109\n",
      "Epoch: 34 Loss G.: 7.174457550048828\n",
      "Epoch: 35 Loss D.: 0.0024400444235652685\n",
      "Epoch: 35 Loss G.: 8.662410736083984\n",
      "Epoch: 36 Loss D.: 0.0002561761357355863\n",
      "Epoch: 36 Loss G.: 8.94770336151123\n",
      "Epoch: 37 Loss D.: 0.0009212191798724234\n",
      "Epoch: 37 Loss G.: 7.855839729309082\n",
      "Epoch: 38 Loss D.: 0.0009961622999981046\n",
      "Epoch: 38 Loss G.: 9.45386028289795\n",
      "Epoch: 39 Loss D.: 0.00016793944814708084\n",
      "Epoch: 39 Loss G.: 9.088438034057617\n",
      "Epoch: 40 Loss D.: 0.0011620203731581569\n",
      "Epoch: 40 Loss G.: 8.275627136230469\n",
      "Epoch: 41 Loss D.: 0.0004401530895847827\n",
      "Epoch: 41 Loss G.: 9.359161376953125\n",
      "Epoch: 42 Loss D.: 0.003664886113256216\n",
      "Epoch: 42 Loss G.: 10.235994338989258\n",
      "Epoch: 43 Loss D.: 0.0002600889129098505\n",
      "Epoch: 43 Loss G.: 11.882326126098633\n",
      "Epoch: 44 Loss D.: 0.0004946651752106845\n",
      "Epoch: 44 Loss G.: 10.203968048095703\n",
      "Epoch: 45 Loss D.: 6.405754538718611e-05\n",
      "Epoch: 45 Loss G.: 11.448875427246094\n",
      "Epoch: 46 Loss D.: 0.0002599488652776927\n",
      "Epoch: 46 Loss G.: 11.863995552062988\n",
      "Epoch: 47 Loss D.: 8.673328920849599e-06\n",
      "Epoch: 47 Loss G.: 12.27806282043457\n",
      "Epoch: 48 Loss D.: 0.013581422157585621\n",
      "Epoch: 48 Loss G.: 12.46721076965332\n",
      "Epoch: 49 Loss D.: 4.976698983227834e-05\n",
      "Epoch: 49 Loss G.: 14.436792373657227\n",
      "Epoch: 0 Loss D.: 0.46197354793548584\n",
      "Epoch: 0 Loss G.: 0.7146005630493164\n",
      "Epoch: 1 Loss D.: 0.2719865143299103\n",
      "Epoch: 1 Loss G.: 0.9962876439094543\n",
      "Epoch: 2 Loss D.: 0.24408704042434692\n",
      "Epoch: 2 Loss G.: 1.314908742904663\n",
      "Epoch: 3 Loss D.: 0.25304609537124634\n",
      "Epoch: 3 Loss G.: 1.5720031261444092\n",
      "Epoch: 4 Loss D.: 0.07938601076602936\n",
      "Epoch: 4 Loss G.: 2.3623223304748535\n",
      "Epoch: 5 Loss D.: 0.10180339962244034\n",
      "Epoch: 5 Loss G.: 3.3844213485717773\n",
      "Epoch: 6 Loss D.: 0.09412530809640884\n",
      "Epoch: 6 Loss G.: 3.1886203289031982\n",
      "Epoch: 7 Loss D.: 0.05523908510804176\n",
      "Epoch: 7 Loss G.: 3.960172176361084\n",
      "Epoch: 8 Loss D.: 0.04371640831232071\n",
      "Epoch: 8 Loss G.: 4.0186896324157715\n",
      "Epoch: 9 Loss D.: 0.09193353354930878\n",
      "Epoch: 9 Loss G.: 4.006129264831543\n",
      "Epoch: 10 Loss D.: 0.06613106280565262\n",
      "Epoch: 10 Loss G.: 4.030298233032227\n",
      "Epoch: 11 Loss D.: 0.016281239688396454\n",
      "Epoch: 11 Loss G.: 4.461764812469482\n",
      "Epoch: 12 Loss D.: 0.07938069850206375\n",
      "Epoch: 12 Loss G.: 4.358641147613525\n",
      "Epoch: 13 Loss D.: 0.016233772039413452\n",
      "Epoch: 13 Loss G.: 5.16751766204834\n",
      "Epoch: 14 Loss D.: 0.007291762158274651\n",
      "Epoch: 14 Loss G.: 4.964371204376221\n",
      "Epoch: 15 Loss D.: 0.00897237379103899\n",
      "Epoch: 15 Loss G.: 5.145219326019287\n",
      "Epoch: 16 Loss D.: 0.009078409522771835\n",
      "Epoch: 16 Loss G.: 5.478028297424316\n",
      "Epoch: 17 Loss D.: 0.011818193830549717\n",
      "Epoch: 17 Loss G.: 6.74397087097168\n",
      "Epoch: 18 Loss D.: 0.010243508033454418\n",
      "Epoch: 18 Loss G.: 8.567218780517578\n",
      "Epoch: 19 Loss D.: 0.08900929987430573\n",
      "Epoch: 19 Loss G.: 4.871605396270752\n",
      "Epoch: 20 Loss D.: 0.02086642198264599\n",
      "Epoch: 20 Loss G.: 5.833606719970703\n",
      "Epoch: 21 Loss D.: 0.0025193174369633198\n",
      "Epoch: 21 Loss G.: 8.316855430603027\n",
      "Epoch: 22 Loss D.: 0.021542038768529892\n",
      "Epoch: 22 Loss G.: 5.230792999267578\n",
      "Epoch: 23 Loss D.: 0.012647719122469425\n",
      "Epoch: 23 Loss G.: 5.902266502380371\n",
      "Epoch: 24 Loss D.: 0.003501842962577939\n",
      "Epoch: 24 Loss G.: 5.522727012634277\n",
      "Epoch: 25 Loss D.: 0.006539067719131708\n",
      "Epoch: 25 Loss G.: 7.3801655769348145\n",
      "Epoch: 26 Loss D.: 0.013094604015350342\n",
      "Epoch: 26 Loss G.: 5.976188659667969\n",
      "Epoch: 27 Loss D.: 0.014218735508620739\n",
      "Epoch: 27 Loss G.: 7.241562366485596\n",
      "Epoch: 28 Loss D.: 0.09523539245128632\n",
      "Epoch: 28 Loss G.: 10.183354377746582\n",
      "Epoch: 29 Loss D.: 0.006653359159827232\n",
      "Epoch: 29 Loss G.: 10.444231033325195\n",
      "Epoch: 30 Loss D.: 0.012565667741000652\n",
      "Epoch: 30 Loss G.: 6.870230674743652\n",
      "Epoch: 31 Loss D.: 0.0030830379109829664\n",
      "Epoch: 31 Loss G.: 6.962086200714111\n",
      "Epoch: 32 Loss D.: 0.0018618602771311998\n",
      "Epoch: 32 Loss G.: 8.285731315612793\n",
      "Epoch: 33 Loss D.: 0.10307526588439941\n",
      "Epoch: 33 Loss G.: 7.507996082305908\n",
      "Epoch: 34 Loss D.: 0.0019493921427056193\n",
      "Epoch: 34 Loss G.: 7.551786422729492\n",
      "Epoch: 35 Loss D.: 0.010522328317165375\n",
      "Epoch: 35 Loss G.: 7.665853977203369\n",
      "Epoch: 36 Loss D.: 0.0069096810184419155\n",
      "Epoch: 36 Loss G.: 7.651393890380859\n",
      "Epoch: 37 Loss D.: 0.03726073354482651\n",
      "Epoch: 37 Loss G.: 7.969799995422363\n",
      "Epoch: 38 Loss D.: 0.0005024612182751298\n",
      "Epoch: 38 Loss G.: 9.763847351074219\n",
      "Epoch: 39 Loss D.: 0.005319919437170029\n",
      "Epoch: 39 Loss G.: 9.794395446777344\n",
      "Epoch: 40 Loss D.: 0.053488366305828094\n",
      "Epoch: 40 Loss G.: 8.90781307220459\n",
      "Epoch: 41 Loss D.: 0.00019057351164519787\n",
      "Epoch: 41 Loss G.: 12.765788078308105\n",
      "Epoch: 42 Loss D.: 0.004264841787517071\n",
      "Epoch: 42 Loss G.: 9.972002029418945\n",
      "Epoch: 43 Loss D.: 0.00024195232253987342\n",
      "Epoch: 43 Loss G.: 10.400197982788086\n",
      "Epoch: 44 Loss D.: 0.014333209954202175\n",
      "Epoch: 44 Loss G.: 11.724847793579102\n",
      "Epoch: 45 Loss D.: 0.006998118013143539\n",
      "Epoch: 45 Loss G.: 14.97161865234375\n",
      "Epoch: 46 Loss D.: 0.010540463030338287\n",
      "Epoch: 46 Loss G.: 11.498661041259766\n",
      "Epoch: 47 Loss D.: 0.000216572530916892\n",
      "Epoch: 47 Loss G.: 15.598785400390625\n",
      "Epoch: 48 Loss D.: 0.0002223002229584381\n",
      "Epoch: 48 Loss G.: 11.639328002929688\n",
      "Epoch: 49 Loss D.: 0.0031676096841692924\n",
      "Epoch: 49 Loss G.: 10.630191802978516\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(TORCH_SEED)\n",
    "DataGroups = namedtuple(\"DataGroups\", [\"all\", \"fraud\", \"valid\"])\n",
    "dg = DataGroups(CustomPandasTorch(fraud_data), CustomPandasTorch(fraud_data.query(\"Class == 1\")), CustomPandasTorch(fraud_data.query(\"Class == 0\")))\n",
    "\n",
    "#Loop 1:\n",
    "#   records: all (21,550)\n",
    "#   training: discriminator only\n",
    "#Loop 2:\n",
    "#   records: fraudulent only (346)\n",
    "#   training: discriminator+generator (GAN)\n",
    "#Within each loop:\n",
    "#   Recall is calculated for:\n",
    "#        1. all records (loop 1: 21,550 | loop 2: 346)\n",
    "#        2. 25% of the records after 75% (loop 1: 16,162 | loop 2: 259) were used for training \n",
    "\n",
    "#This cell takes ~2.5-4mins to run\n",
    "\n",
    "result_lst = []\n",
    "\n",
    "for i, (t, func) in enumerate(zip([dg.all, dg.fraud], [train_discriminator, train_gan])):\n",
    "    print(i)\n",
    "\n",
    "    #Set active dataset to raw\n",
    "    t.set_raw()\n",
    "\n",
    "    #All records recall score\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True) \n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.active.data), t.active.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"all\"])\n",
    "    \n",
    "    #Set active dataset to train\n",
    "    t.set_train() \n",
    "\n",
    "    #25% test records recall score\n",
    "    train_loader = torch.utils.data.DataLoader(t, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    discriminator, generator = func(train_loader)\n",
    "    result_lst.append([recall_calc(discriminator(t.test.data), t.test.labels), t.active.sample.shape[0], t.active.df.shape[0], discriminator, generator, \"train\"])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>df_size</th>\n",
       "      <th>disc</th>\n",
       "      <th>gen</th>\n",
       "      <th>df_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.7940)</td>\n",
       "      <td>43136</td>\n",
       "      <td>21550</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.8040)</td>\n",
       "      <td>32384</td>\n",
       "      <td>16162</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>None</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.9944)</td>\n",
       "      <td>16000</td>\n",
       "      <td>346</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.9625)</td>\n",
       "      <td>16000</td>\n",
       "      <td>259</td>\n",
       "      <td>Discriminator(\\n  (model): Sequential(\\n    (0...</td>\n",
       "      <td>Generator(\\n  (model): Sequential(\\n    (0): L...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           recall  sample_size  df_size  \\\n",
       "0  tensor(0.7940)        43136    21550   \n",
       "1  tensor(0.8040)        32384    16162   \n",
       "2  tensor(0.9944)        16000      346   \n",
       "3  tensor(0.9625)        16000      259   \n",
       "\n",
       "                                                disc  \\\n",
       "0  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "1  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "2  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "3  Discriminator(\\n  (model): Sequential(\\n    (0...   \n",
       "\n",
       "                                                 gen df_type  \n",
       "0                                               None     all  \n",
       "1                                               None   train  \n",
       "2  Generator(\\n  (model): Sequential(\\n    (0): L...     all  \n",
       "3  Generator(\\n  (model): Sequential(\\n    (0): L...   train  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(result_lst, columns = [\"recall\", \"sample_size\", \"df_size\", \"disc\", \"gen\", \"df_type\"])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=29, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training for best discriminator:\n",
    "#1. 75% of all fraudulent records were used to train (other 25% used later for testing along with 100% of valid records)\n",
    "#2. Used resampling to upsample fraudulent training records to 16,000 (up from 259)\n",
    "#3. Used GAN model to generate extra fraduluent training records for improved recall\n",
    "best_discriminator = df_results[\"disc\"][3]\n",
    "best_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9540)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing best discriminator:\n",
    "#1. Test set includes 25% of the fraudulent records (not used for training) and all valid records\n",
    "#2. Final Test Data Recall Calculation with best Discriminator below (i.e., best_discriminator)\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "train_idx = dg.fraud.train.df.index\n",
    "X_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,:-1]\n",
    "y_test = torch.from_numpy(dg.all.raw.df.query(\"index not in @train_idx\").astype('float32').values)[:,-1:]\n",
    "recall_calc(best_discriminator(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9669)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_idx = dg.fraud.train.df.index\n",
    "# # t_test = CustomPandasTorch(dg.all.active.df.query(\"index not in @train_idx\"))\n",
    "# t_test = CustomPandasTorch(dg.all.raw.df.query(\"index not in @train_idx\"))\n",
    "# recall_calc(result_lst[3][3](t_test.active.data), t_test.active.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further Work TODO:\n",
    "\n",
    "#1. Resample Data before adding to discriminator DONE\n",
    "#2. Add in Generator DONE\n",
    "#3. Standardize the Amount feature \n",
    "#4. Increase Number of Epochs\n",
    "#5. Analyze ROC and recall-precision curves\n",
    "#6. Hyperparametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Accuracy, Recall Visual\n",
    "![alt text](precision_acc_recall.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_NYCDSA",
   "language": "python",
   "name": "v_nycdsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
